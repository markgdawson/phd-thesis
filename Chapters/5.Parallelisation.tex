\label{Chapter5}
\chapter{Parallelisation} % Write in your own chapter title
\lhead{Chapter 5. \emph{Parallelisation}} % Write in your own chapter title to set the page header

*** 
The weak form of Maxwells equations means that parallelisation strategies are possibile.

The Message Passing Interface (MPI) is used and a *** strategy with distributed memory is utilised. The parallel computation is performed by a process of data decomposition. This means that the computational domain is partitioned into regions of space and each processor performs computations only on the elements in that region. This is made possible by the form of the weak form given in \eqref{maxwell-DG-weak-form}. We note that the weak form involves primarily values of unknowns on a single element. The only exception being the values of the solution $U_{out}$ from neighbouring elements required for the calculation of the numerical flux, $\tilde{\mathbf{F}}_n$ along element faces. These values should be made available to each element by communication between processors at each timestep.

\section{Domain Decomposition}

Let us consider calculation of updated solution values, $U^{n+1}_i$, from solution values at the previous timestep, $U^{n}_i$ for a single element $\Omega_i$. We associate a given parition $\Omega^p$ with a processor $P^p$, and calculations of the associated values of $U^{n+1}_i$ will occur on processor $P^p$.

Assume that the open bounded domain $\Omega \subset \R^{n}$ with a boundary $\partial \Omega$ is partitioned in the usual way for finite elements into a regular paritioning $\bar{\Omega} = \cup_{e} \bar{\Omega}_e$, such that $\Omega_i \cap􏰰\Omega_j = \O$, for $i \ne j$. The boundary $\partial \Omega$ is given by $\partial \Omega = \Gamma^{PEC} \cup \Gamma^{ABC}$ where $\Gamma^{PEC}$ and $\Gamma^{ABC}$ are repectively the boundaries along which PEC and ABC boundary conditions are prescribed and $\Gamma^{PEC} \cap \Gamma^{ABC} = \O$. We define the internal faces $\Gamma^{I} = ( \cup_e \partial \Omega_{e}) \ \partial \Omega$.

We wish to perform calculations in parallel by dividing the domain $\Omega$ into regions of elements $\Omage^p$ assigning computations of elements to each processors.

The domain $\Omega$ is also divided with a regular partitioning $\bar{\Omega} = \cup_{p} \bar{\Omega}^p$ with partition boundary $\partial \Omega^{p}$. Each non-overlapping partition $\Omega^p$ is composed of a number of elements such that $\bar{\Omega}^p = \cup_{e} \bar{\Omega}_e$ and $\Omega^i \cap􏰰\Omega^j = \O$, for $i \ne j$.
% WHEN does U_out need to be available? Prior to compution? U_out is actually the value calculated at the previous time step.

For each partition $\Omega^p$ we refer to its internal and external interface elements respectively as $L^p_{int} = \{ \Omega_e \subset \Omega \setminus \Omega^p | \partial \bar{\Omega}_e \cap \partial \bar{\Omega}^P \setminus\partial \bar{\Omega}\ne \O \}$ and $L^p_{ext} = \{ \Omega_e \subset \Omega^p | \partial \bar{\Omega}_e \cap \bar{\Omega}^P \ne \O \}$.


Calculating $U^{n+1}_i$ for all elements $\Omega_i \in \Omega^{p}$, requires values of $U_i^{n}$ for all elements $\Omega_i \in \Omega^{p} \cup L^p_{ext}$. For all elements in $\Omega^p$ these are readily available on $P^p$ from calculations in the previous time step, however for elements in $L^p_{ext}$, $U^{n}_i$ will be calculated on a processor other than $P^p$, and should be duplicated on this processor.
% I've not mentioned (really) any correspondence between L^p_{out} and L^p_{in}
% talk about types of these elements and illustrate further with examples (figures)

For each element $\Omega_i \in L^p_{in}$ we define its adjacent external elements as $K_{i} = \{ \Omega_i \subset \Omega \ \Omega^p | \partial \bar{\Omega}_i \cap \partial \bar{\Omega}_i \}$.
% this could also be done as \Omega_i \subset L^p_{out} ?? How to I say that 
The values of $U_i^{n}$ for each element $\Omega_i$ are sent to all processors which containing one of more elements in $K_{i}$.  Conversely each processor $P^p$ recieves values of each $\Omega_e \in L^p_{ext}$ from the processor to which $\Omega_e$ was assigned.

\subsubsection{Implementation detail}

Communication is an expensive operation with a significant overhead associated with each connection established [].
%[how much overhead? How expensive]. 
The number of messages can be reduced by collecting all the values of $U_{n}$ to be sent to any one 
destination processor and sending in a single message per time step.

Additionally should choose $\Omega^p$ such that we minimise both the number of messages and the number of values of $U_{n}$ to communicate.

% other processor has to know who they're from!! Talk about this in implementation
% [note: there could also a minimisation condition on the number of unknowns]
% note: there are a lot of factors at play here - first I want to minimise the number of communications established.....easy - i communicate once up front.
% - minimse the number of elements in $L^p$ (number of intersection of faces)
% - minimise the number of unknowns (weighting)
% - minimise the number of comms

This is equivalent to minimising the number of elements in $\cup_p L^p$ [more here!]. Deciding which elements should be contained in $\Omega^p$ is done using parMETIS [], a public-domain library which implements several algorithms for optimal partitioning of graphs.

The parMETIS library requires that each processor builds an element connectivity graph for a subset of all elements $\Omega_e$, where each element is uniquely referred to by a global numbering shared between all processors. The global numbering scheme established by the meshing software is used. The ranges of element numbers are divided equally amongst the processors by number ranges, not necessarily connected, and a graph built for subset of elements. Each node of the graph represents and element in the initial partition and each vertex represents a shared face or edge of two elements. 

The connectivity information is obtained by first constructing a node connectivity list, which for each node lists the elements which contain that node. This list is constructed by reading the whole mesh connectivity information and associating each read element which all the nodes it contains. The connectivites for all elements is read - however since these are integer values memory usage is not considered relevant [give an example of a very large mesh]. We loop through all faces of the elements allocated to this processor and for each face perform a loop on the nodes within that face. Within the inner loop we construct a list of connected elements [what is this]. The connected face will be the element in this list which appears the same number of times as the number of nodes in the face.

From the connectivity graph, parMETIS can then optimise the partitions in such a way to reduce the number of faces shared across processors. The resulting partition is shown for a circle with 5 processors in \figref{parmetis-figure}

The result of the optimisation by parMETIS is a list of all elements allocated to this processor and a number between 1 and the total number of processors (or equivalently the number of parts into which the domain is partitioned) which indicated the processor on which calculations should be performed (or equivalently the parition in which this element belongs). We allow each partition of the domain to correspond to a partitioning $\Omega^p$.

\section{Communication Arrays and Data Partition}

Recall that in order to implement the parallelised DG method it is not sufficient to distribute computations, some values of solution need to be communicated at each time step. In order to simplify the extension of a serial code to multiple processors the strategy employed involves establishing the set of elements that, for a given processor $A$, share a face with an element in the partition allocated to processor $A$. Nodal solution values for these elements are copied from the processor on which they are calculated to processor $A$ at each time step. The data partition is show in \figref{parmetis-figure} and results in a number local elements on which calculations are performed on this processor and a number of interface elements for which nodal solution values are available without calculation. By this data partition method code to calculate the numerical flux contributions does not need to distinguish betweeen local elements and interface elements.

** I need to talk here about how I set up the communication arrays, what else needs doing? what communication is required in pre-processing stage? etc **
** question: why do I need to keep track of the original numbering scheme? (i.e. localElementMap) **

\begin{figure}[htbp!]
 \centering
 \includegraphics[width=0.3\textwidth]{Figures/Parallelisation/parmetisPartition}
 \includegraphics[width=0.2\textwidth]{Figures/Parallelisation/parmetisDataPartition}
\caption{Left: Partition of a circle given by parMETIS. Each color represents a different processor on which calculations for that processor should be performed. Right: Data partition of elements for a single processor (red) for circle}
\label{parmetis-figure}
\end{figure}

The nodal values to communicate between processors is be established in the preprocessing state, prior to computation. By parsing the graph and communication each processor recieves a list of elements for computation and the processors to which their nodal values should be sent and a list of interface elements ordered by processors number on which calculations occur. The vector of unknowns $U$, is implemented as a 1-dimensional array. By using the sendtoall functions available through the MPI library the elements numbering can be done in such a way that the MPI buffer recieved is a contiguous section of the vector U. this removes the need for a map of nodal values or copying values between the MPI buffer and the vector $U$.

The solution can be advanced in time following the exact procedure as the serial code, for a single partition of the domain per processor, provided that at each stage of the RK4 method the the nodal solution values should are communicated.

\section{Ordering of Faces}
